{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pdf_elements=partition_pdf(\n",
    "    filename=\"data/cj.pdf\",\n",
    "    strategy=\"hi_res\",\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_types=[\"Image\",\"Table\"],\n",
    "    extract_image_block_to_payload=False,\n",
    "    extract_image_block_output_dir=\"extracted_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Header at 0x2c3f7ee89a0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f86eb490>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f7ee8220>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f7ee8ca0>,\n",
       " <unstructured.documents.elements.Text at 0x2c3f77ee410>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f7a099c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f7a090c0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b4c70>,\n",
       " <unstructured.documents.elements.Text at 0x2c3f8b69060>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f7a09030>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f7a08250>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f7a09150>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f7a09690>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f7a0a2f0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f7a09f00>,\n",
       " <unstructured.documents.elements.Text at 0x2c3f8b6bf40>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f7a09d80>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f7a0afb0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f7a0be50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f7a0b7c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f7a0b2b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f7a0b100>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f7a0ace0>,\n",
       " <unstructured.documents.elements.Table at 0x2c3f8619ea0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f8619f60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f861a200>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f861a0b0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f861a350>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f861a4a0>,\n",
       " <unstructured.documents.elements.Table at 0x2c3f861a5f0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f861a740>,\n",
       " <unstructured.documents.elements.Table at 0x2c3f861a890>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f861a9e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f861ac80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f861ab30>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f861af20>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f861add0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f861b070>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f861b1c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f861b310>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f861b460>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f861b5b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f861b700>,\n",
       " <unstructured.documents.elements.Text at 0x2c3f8b68370>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f861b850>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f861baf0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f8b68250>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f861b9a0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f861bee0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b4070>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b41c0>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f77b4310>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b4460>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77b45b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77b4700>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77b4850>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f8b68670>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b49a0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b4d90>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b4ee0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b5030>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f77b5180>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x2c3f77b52d0>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f77b5420>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b5570>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b56c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b5960>,\n",
       " <unstructured.documents.elements.Text at 0x2c3f8b68bb0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b5810>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b5c00>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b5d50>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b5ea0>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f77b5ff0>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f77b6140>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b6290>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b63e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b6680>,\n",
       " <unstructured.documents.elements.Text at 0x2c3f8b69750>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b6530>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b6920>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b6a70>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b6bc0>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f77b6d10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b6e60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b7100>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b7250>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f77b73a0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b74f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b7640>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b7790>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b78e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77b7b80>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77b6fb0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b7a30>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b7cd0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77b7e20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77ec070>,\n",
       " <unstructured.documents.elements.Text at 0x2c3f8b68220>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77ec100>,\n",
       " <unstructured.documents.elements.Text at 0x2c3f8b68280>,\n",
       " <unstructured.documents.elements.Text at 0x2c3f8b682e0>,\n",
       " <unstructured.documents.elements.Text at 0x2c3f95dcc70>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f77ec250>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77ec3a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77ec4f0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77ec640>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77ec790>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77ec8e0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77eca30>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77ecb80>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77eccd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77ece20>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77ecf70>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f77ed0c0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f77ed210>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77ed4b0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77ed360>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77edc90>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77edb40>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77edde0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77edf30>,\n",
       " <unstructured.documents.elements.Table at 0x2c3f77ee080>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dd120>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dce20>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dcd60>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dd450>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dd0c0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dd330>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dd4b0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dd300>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dcf40>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dd570>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dd150>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dcfd0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dce50>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dd600>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f8c9ce20>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f8c9c940>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f8c9c5b0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f8c9c610>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f77ee1d0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77efc10>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f77efd60>,\n",
       " <unstructured.documents.elements.Table at 0x2c3f77efeb0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f90e4b80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f90e7f40>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f90e7df0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f90e7b50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f90e7ca0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Header=[]\n",
    "Footer=[]\n",
    "Title=[]\n",
    "NarrativeText=[]\n",
    "Text=[]\n",
    "ListItem=[]\n",
    "for element in raw_pdf_elements:\n",
    "  if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
    "            Header.append(str(element))\n",
    "  elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
    "            Footer.append(str(element))\n",
    "  elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
    "            Title.append(str(element))\n",
    "  elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "            NarrativeText.append(str(element))\n",
    "  elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
    "            Text.append(str(element))\n",
    "  elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
    "            ListItem.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=[]\n",
    "for element in raw_pdf_elements:\n",
    "  if \"unstructured.documents.elements.Image\" in str(type(element)):\n",
    "            img.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = []\n",
    "for element in raw_pdf_elements:\n",
    "  if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tab.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reported Revenue Next Quarter Rev Actual Consensus A Guidance Consensus A ServiceNow $2,957M $2959.6M (0.1%) NA $3122.0M NA AppFolio $204.0M $199.5M 2.3% NA $217.3M NA Atlassian $1286.5M $1238.0M 3.9% $1349.0M $1311.1M 2.99 Dvnatrace $436.0M $426.5M 2.2% $434.5M $426.5M 1.99',\n",
       " 'EV/NTM~ EV/2026 EV/NTM NTMRev Gross Operating FCF % in To Company Rev Rev FCF Growth Margin Margin Margin OverL 1 Palantir 54.4x 42.4x 137x 25% 81% 14% 37% 99% 2 Cloudflare 23.9x 17.9x 197x 25% 78% (10%) 11% 1009 3 CrowdStrike 20.9x 16.2x 80x 20% 75% (0%) 30% 1009 4 Samsara 19.7x 15.2x 199x 23% 76% (19%) 2% 1009 5 ServiceNow 15.5x 12.9x 49x 20% 79% 12% 31% 1009 6 Datadog 15.0x 11.7x 51x 21% 81% 3% 29% 1009 7 Shopify 14.9x 11.8x 75x 22% 51% 11% 17% 23% 8 Guidewire 14.4x 12.3x 75x 16% 61% (2%) 18% 34% 9 Snowflake 13.8x 10.7x 55x 22% 67% (39%) 23% 72% 10 HubSpot 13.4x 11.0x 76x 15% 85% (3%) 16% 41% Clouded Judgement @jaminball ALTIME',\n",
       " '7 Day Share 30DayShare YTDShare Current Mark Company Price A Price A Price A Cap ($MM) 1 Agora 33% 45% 45% $552 2 Twilio 31% 37% 37% $22,754 3. Shopify 12% 12% 12% $153,937 4 Cloudflare 11% 27% 27% $47,275 5 Zoom 9% T% T% $26,864 6 GitLab 9% 23% 23% $11,260 7 Samsara 8% 19% 19% $29,177 8 Digital Ocean 8% 22% 22% $3,848 9 Klaviyo 8% 15% 15% $12,894 10 Domo 7% 17% 17% $323 Clouded Judgement @jaminball ALTIMETE',\n",
       " 'Cloudfiare CrowdStrike Samsara ServiceNow Datadog Guidewire Soowfiake HubSpot ServiceTitan Alto Atiassian OneStream Autodesk AppFolio Monday.com Dynatrace Mongod8 Confluent Salesforce Hashicorp SentinelOne Workday Paylocity Wix.com Workiva Valuation Rev Multiple FCF Muhkiple Rev Growth Market Cap EV L™ NT™ 2026 LT™ NTM L™ NTM $184,955 $180,739 68.3x 54.4x 42.4x 184x 137x 25% 25% $47,275 $46,902 29.8x 23.9% 17.9% 276x 197x 30% 25% $97,760 $94,330 25,2x 20,9x 16,2x 85x 80x 31% 20% $29,177 $28,591 24.2x 19.7x 15.2x 1505x 199x 39% 23% $208,627 $205,143 18.7x 15,5x 12,9x 60x 49x 22% 20% $48,381 $46,152 18,2x 15,0x 11,7x 63x 51x 26% 21% $153,937 $150,183 18.3x 14.9x 11.8x 105x 75x 23% 22% $17,667 $17,312 16.7x 14.4% 12.3x 93x 75x 13% 16% $59,184 $57,627 16.9% 13,8x 10,7x 73x 55x wW% 22% $39,853 $38,673 15.4x 13.4x 11.0x 99x 76x 22% 15% $13,714 $13,410 16.7x 12,9x 9.7x NM 435x 37% 20% $9,025 $10,833 15.0x 12,.6x NA 554x 417x 24% 18% $122,985 $116,500 14.1x 12.3x 10.3x 38x 32x 15% 14% $69,552 $68,327 14.2 12.3 10.2x 52x 44x 23% 16% $6,992 $6,645 14,5x 11,9x 11,5« NA 116x 31% 21% $11,260 $10,389 14.6x 11.7x 9.0x NM 71x 32% 25% $38,488 $33,497 12.6x 11.3x 9.7x 31x 28x 16% 12% $12,894 $12,110 13.9% 11, 4x 8.6x 94x 79x 35% 26% $31,122 $29,651 12.9x 10.8 8.5x 45x 45x 31% 20% $66,639 $67.240 11.3x 10, 1x 8.8x Six 36x 12% 12% $9,216 $8,926 11.2x 9.7x 8.3« 52x 37x 28% 16% $11,833 $10,536 11,6x 9.2x 7.0x 38x 32x WU% 27% $11,908 $11,173 10.1x 9.0x 77x 72x 64x 24% 12% $17,293 $16,361 10.0x 8.9x 7.6x 40x 35x 20% 13% $20,187 $19,087 10,0x 8.6x 69x 129% 110x 21% 16% $194,144 $192,314 8.9x 8.2x 74x 24x 21x 11% 9% $9,772 $9,026 9.9x 8,2x 64x NM 150x 25% 20% $328,796 $328,217 8.8x 8.1x 7.3x 28x 26x 10% 9% $4,137 $3,476 10.0x 8.1x 64x NM NM 22% 24% $5,901 $6,035 8.9x 8.0x 7.0x 61x 53x 11% 11% $7,002 $5,738 8.8x 7.7* 6.5x 100x 66x 16% 14% $3,945 $3,493 8.5x 7.3x 6.0x 38x 36x 24% 17% $5,113 $4,587 7.7x 7.2x 6.6x 21x 19x 10% 8% $7,751 $6,901 9.0x 71x 5.5x 1372x 103x 34% 26% $69,179 $65,384 8.0x 71x 6.0x 31x 27x 17% 13% $11,462 $10,853 7.9x 7.0x 6.0x S4x 50x 19% 12% $11,231 $10,831 7.7x 6.9x 6.0x 3ix 30x 9% 12% $13,288 $13,293 7.8x 6.8x 5.8x 31x 23x 13% 15% $5,489 $5,503 7.8x 6.7 5.6x 82x 49x 16% 16% $4,746 $4,345 7.7% 6.6x 5.4x 4893x 97x 28% 17% $4,002 $3,819 7.3K 6.4% 5.4x 53x 35x 13% 14% LT™ Rovenue $2,646 $1,572 $3,740 $1,179 $10,984 $2,536 $8,212 $1,036 $3,414 $2,506 $803 $724 $8,288 $4,795 S460 $712 $2,656 $869 $2,299 $5,961 $794 $907 $1,110 $1,634 $1,916 $21,505 $916 $37,189 $347 $676 $655 $410 $593 $8,157 $1,376 $1,403 $1,704 $705 $523 Gross Margin _ 81% 78% 75% 76% 79% 81% 51% 61% 67% 85% 69% 64% 74% 82% 74% 78% 78% 91% 82% 81% 74% 89% S823 82% 74% 76% 74% 69% 68% 60% Operating Margin _ 14% (10%) (0%) (19%) 12% 3% 11% (2%) (39%) (3%) (137%) (22%) 9% (3%) NA (22%) 24% (10%) (5%) 22% 18% (4%) (10%) 10% (14%) 36% (43%) 20% (90%) (7%) (29%) (20%) W% (43%) 5% (8%) 19% 4% (10%) (24%) (3%) FCF Margin Rule of 40 LTM Operating Expenses % Rev Net GM Adj. LIM. NT. LT NT™ Sam _R&0____ GEA SBC ___ Expansion _ Payback _ 37% 38% 62% 63% 30% 17% 20% 20% NA NA 11% 11% 41% 3% 46% 25% 17% 20% 110% 22 Months 30% 26% 61% 47% 38% 26% 12% 21% 115% 37 Months 2% 10% 41% 33% 49% 25% 20% 23% 115% 28 Months 31% 32% 54% 52% 35% 23% 9% 16% NA 24 Months 29% 28% 55% 49% 28% 43% 7% 21% 115% 16 Months 17% 18% WA% 41% 17% 16% 5% 5% NA NA 18% 19% 31% 35% 20% 27% 17% 14% NA NA 23% 25% 54% 47% 47% 48% 11% 40% 127% 26 Months 16% 17% 37% 32% 47% 23% 12% 19% NA 33 Months (5%) 3% 32% 32% 104% 64% 39% 103% 120% 32 Months (2%) 3% 22% 21% 32% 33% 20% 14% 110% 43 Months 37% 39% 52% 53% A% 23% 8% 13% NA NA 29% 28% 52% 44% 20% 50% 13% 25% NA 11 Months NA 10% NA 31% NA NA NA NA NA 41 Months (15%) 16% 17% 41% 53% 32% 26% 26% 124% 27 Months 42% 41% 58% 52% 15% 26% 9% 16% NA 21 Months 8% 13% 44% 39% 42% 22% 15% 16% 110% 29 Months 28% 24% 59% 44% 50% 24% 9% 24% 114% 33 Months 22% 28% 34% 40% 33% 24% 11% 11% 105% NA 23% 26% 51% 42% 13% 20% 11% 8% NA NM 31% 28% 65% 54% 57% 21% 15% 13% 111% 29 Months 14% 14% 38% 26% 46% 27% 19% 16% NA 41 Months 25% 25% 45% 38% 36% 22% 12% 16% 111% 70 Months 8% 8% 29% 23% 45% 31% 12% 26% 120% 18 Months 37% 39% 47% 49% 27% 18% 7% 9% NA NA (1%) 5% 24% 26% 57% 43% 17% 42% 117% 35 Months 32% 32% 41% 40% 36% 14% 7% 8% NA 108 Months (17%) (9%) 5% 15% 66% 60% 24% 64% NA 33 Months 15% 15% 26% 26% 16% 21% 18% 13% NA 78 Months 9% 12% 25% 25% 55% 34% 23% 25% 109% 37 Months 22% 20% 47% 37% 44% 36% 17% 29% 117% 30 Months 37% 37% 47% 45% 21% 19% 1% 13% NA NA 1% 7% 35% 33% 60% 32% 24% 32% NA 36 Months 26% 26% 43% 39% 29% 32% 10% 18% NA 43 Months 15% 14% 33% 26% 43% 26% 12% 18% 112% 37 Months 25% 22% 34% 34% 24% 13% 13% 10% NA NA 25% 29% 38% 44% 25% 29% 10% 14% NA NA 10% 13% 26% wW% 47% 26% 14% 14% 111% 32 Months 0% 7% 28% 24% 49% 23% 20% 20% 113% 47 Months 14% 18% 27% 33% 23% 24% 16% 13% NA 25 Months Shore Price Performat Current $81 $137 $397 $52 $1,013 $143 $119 $212 $179 $772 $74 $100 $187 $267 $30 $69 $237 S47 $203 $310 $254 $242 $80 $58 $271 $446 $30 $344 $32 $98 Su $35 $140 $24 $260 $111 $201 $242 $39 $46 $35 % Week 3% 11% 5% 8% (11%) 2% 12% 3% 2% 5% 5% 0% 0% 0% (2%) 9% 6% 8% 5% 3% (3%) (3%) 2% 5% 4% 2% 4% 3% (6%) 3% 2% (2%) 4% 2% 6% (1%) 2% (1%) 6% % 30 7% 27% 16% 23% 13% 15% 5% 3% 3% 7% 6% 16% 6% (7%) (3%) 20% (0%) 1% 1% 13% (9%) 3%',\n",
       " 'Valuation Rev Multiple FCF Multiple Rev Growth LT™ Gross Operating FCF Margin Rule of 40 Market Cap EV LT™ NTM 2026 LT™ NT™ LT NT Revenue Margin Margin LT NTM L™ NT™M Bil.com $9,948 $9,450 7.0x 6.3x §.2x 38x 40x 19% 12% $1,344 82% (7%) 20% 15% 39% 27% Freshworks $5,806 $4,782 7.0x 6.1x $.2x 34x 24x 20% 15% $686 84% (23%) 20% 25% 41% 40% Asana $4,885 $4,699 6.6x 6.0x 5.2x NM 155x 12% 10% $707 83% (38%) (4%) 4% 8% 14% DocuSign $19,364 $18,552 64x 6.0x 5.5x 21x 21x 8% 6% $2,913 79% 6% 31% 29% 38% 35% Unity $9,269 $10,464 5.3x 6.0x S.4x 43x 37x (3.1%) (11%) = $1,965 68% (45%) 12% 16% 9% 5% Digital Ocean $3.848 $5,033 6.7x 5.9x 5.0x 53x 34x 12% 12% $757 60% 9% 13% 17% 25% 29% BlackLine $3,964 $4,065 6.4x 5.9x 5.2x 25x 24x 11% 0% $640 75% 4% 25% 24% 37% 32% Paycom $11,836 $11,591 64x 5.8x 5.2x 37x 32x 12% 10% $1,824 82% 32% 17% 18% 29% 28% Okta $16,066 $14,880 §.9x 54x 4.9x 24x 22x 17% 8% $2,533 76% (5%) 24% 25% 41% 33% Tenable $5,192 $5,051 5.8x 5.3x 4.7x 31x 20x 14% 0% $878 78% (3%) 19% 26% 33% 35% Box $4,775 $5,331 5.0x 47x 43x 18x 17x 4% 6% $1,073 78% 8% 28% 28% 32% 34% Twilio $22,754 $21.182 4.9% 45x 41x 29x 28x 6% 9% $4,339 51% (2%) 17% 16% 23% 24% Dropbox $9,952 $11,084 44x 43x 4.3x 12x 12x 3% 0% $2,540 82% 20% 35% 37% 38% 38% UiPath $8,013 $6,491 46x 4.2x 3.7x 21x 18x 17% 9% $1,411 83% (13%) 22% 23% 38% 32% Appian $2,599 $2,778 4.7x 4.2« 3.7x NM 162x 13% 11% $596 75% (14%) (3%) 3% 10% 14% Zoominfo $3,595 $4,882 4.0x 44x 4.0x 15x 12x (0%) (3%) $1,222 85% 12% 27% 33% 27% 30% Sprout Social $1,907 $1,832 4.7x 4.1« 3.4x 91x 36x 27% 14% $392 77% (17%) 5% 19% 32% 25% Zoom $26,864 $19,228 4.2x 4.0x 3.9x 11x 12x 3% 3% $4,628 76% 16% 37% 33% 40% 35% Amplitude $1,523 $1,208 4.1% 3.8x 3.4x 103x 60x 8% 8% $283 74% (32%) 4% 6% 12% 14% Couchbase $910 $774 3.8x 34x 2.8x NM NM 19% 11% $205 88% (39%) (15%) (2%) 4% 9% PagerDuty $1,663 $1,597 3.5x 3.2x 2.8x 16x 20x 9% 9% $457 82% (18%) 22% 16% 30% 25% Jamf $1,978 $2,152 3.5x 3.2x 2.7% 71x 15x 14% 11% $615 77% (13%) 5% 21% 19% 32% Zvora $1,629 $1,479 3.3x 3.1x 2.8x 21x 18x 7% 6% $453 67% (9%) 16% 18% 23% 24% Fived $3,065 $3,336 3.3x 3.0x 2.7x 49x 24x 14% 10% $1,002 53% (7%) 7% 12% 21% 23% Fastly $1.505 $1,611 3.0x 2.9% 2.6x NM NM 11% 3% $841 55% (30%) (6%) (0%) 5% 3% Ole $1,241 $906 3.3x 2.9% 24x 40x 35x 26% 16% $272 56% (12%) 8% 8% 35% 23% Sprinkic $2,259 $1,833 2.3x 2.3x 2.1x 26x 26x 12% 3% $788 73% 5% 9% 9% 21% 12% Agora $552 $272 2.0x 1.9x 1.7x nM” Ox (7%) 4% $135 63% (42%) (34%) 10% (41%) 14% RingCentral $3,203 $4,771 2.0x 1.9x 1.7x 12x 10x 9% 7% $2,357 70% (2%) 17% 19% 27% 26% Yext $849 $847 2.1x 18x 1.8% 22x 16x 1% 12% $409 78% (5%) 9% 12% 10% 24% Kattura $343 $314 1.8x 17x 1,7x 38x 25x 2% 1% $178 65% (16%) 5% 7™% 6% 8% Riskified $887 $554 17x 1.7x« 14x 16x 12x 9% 4% $318 54% (16%) 11% 13% 20% 17% CS Disco $309 $240 17x 1.6x 1.5x NM NM 6% 4% $144 75% (30%) (8%) (9%) (1%) (6%) BigCommerce $489 $543 1.6x 1.6x 14x 23x 19x 11% 4% $330 77% (10%) 7% 8% 18% 13%']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oo B D1 Gi',\n",
       " '20.0x 10.0x 5.0x 0.0x ( & S & of SP & 9 fF PF Pr DH PD PM Pe y yr Ye fey w Y yr a Ds) wo ys Yy yw o w ¥ —— Median LT Pre Covid Average —— 10 Year Treasury Clouded Judgement @jaminball Source: Bloomberg / Pitchbook consensus estimates ALT IMET',\n",
       " '90.0x 80.0x 70.0x 60.0x 50.0x 40.0x 30.0x 20.0x 10.0x 0.0x & se é f « Pa a Rad & £ an 3 ss Ps & —— Median ——Top 5 Median Clouded Judgement @jaminball Source: Bloomberg / Pitchbook consensus estimates ALTIMET',\n",
       " '40.0x 35.0x 30.0x 25.0x 20.0x 15.0x 10.0x 5.0x 4 0.0x > 2 oy we , 2 © ye 2P 2 PB DP DD DR Dh Dm DP om oD F os cay » iy Sy, 2 $ , wv $ : é al ‘ ” a 2 wt of we Ss FF F KF KT KS SK YL we we & ——=High Growth Median === (Vid Growth Median —— Low Growth Median Clouded Judgement @jaminball Source: Bloomberg / Pitchbook consensus estimates ALTIMET',\n",
       " 'Clouded Judgement @jaminball',\n",
       " '1.0x 0.9x 0.8x 0.7x 0.6x 0.5x 0.4x 0.3x 0.2x 0.1x 7) ) < o a) ev > 2 2 oO J) y ‘vy 1 > > b& Rs “ oe s ris »” s x se ad a” Pd & a a x ad x ye RY oo ¥ » < g ¥ x Ss x < » or fx ib.) == Median EV / Rev / Growth ——LT Average —— 10 Year Treasury Clouded Judgement @jaminball Source: Bloomberg / Pitchbook consensus estimates ALTIME1',\n",
       " 'Growth Adjusted EV / NTM Revenue (EV / NTM Rev / NTM Growth) Median: 0.6 Iii it iii 0.0% ServiceNow Clouded Judgement @jaminball',\n",
       " '70.0x 60.0x 50.0x 40.0x aC 30.0x 20.0x 10.0x -Ox 1/1/15 1/1/16 1/1/17 1/1/18 1/1/19 1/1/20 1/1/21 1/1/22 1/1/23 1/1/24 1/1/25 === FCF Positive & <100x Multiple Median ———LT Average',\n",
       " 'EV / NTM FCF 250% 200% 150x 100x 50x z & Clouded Judgement @jaminball ALT',\n",
       " '@cnwo Pail ie 20x e 7” y = 60.879x + 0.2737 *, - i R? = 0.3849 J wre 3 0006 | suop . 10x 0% 5% 10% 15% 20% 25% 30% Clouded Judgement @jaminball ALTIM']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_pdf_elements2=partition_pdf(\n",
    "    filename=\"data2/2005.11401v4.pdf\",                 \n",
    "    strategy=\"hi_res\",                                 \n",
    "    extract_images_in_pdf=True,                      \n",
    "    extract_image_block_types=[\"Image\",\"Table\"],         \n",
    "    extract_image_block_to_payload=False,                  \n",
    "    extract_image_block_output_dir=\"extracted_data2\",  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Text at 0x2c3853b31f0>,\n",
       " <unstructured.documents.elements.Text at 0x2c3853b2bc0>,\n",
       " <unstructured.documents.elements.Text at 0x2c3853b24a0>,\n",
       " <unstructured.documents.elements.Text at 0x2c3853b0970>,\n",
       " <unstructured.documents.elements.Text at 0x2c3853b1510>,\n",
       " <unstructured.documents.elements.Header at 0x2c3853b10f0>,\n",
       " <unstructured.documents.elements.Text at 0x2c3853b0ac0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b0880>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b0a00>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b27a0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b2740>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b3fd0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b3130>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b1210>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b1990>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b2b00>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b0f40>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b2770>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b0820>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b2020>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b0730>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b33d0>,\n",
       " <unstructured.documents.elements.Image at 0x2c3853b2710>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b1600>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b2650>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b1ea0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b06a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c385081ae0>,\n",
       " <unstructured.documents.elements.Title at 0x2c385081bd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3850819f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c385083a00>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3850806d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c385080760>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c385081390>,\n",
       " <unstructured.documents.elements.Title at 0x2c3850804f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c385081ff0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b39d0>,\n",
       " <unstructured.documents.elements.Formula at 0x2c385082200>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c385080910>,\n",
       " <unstructured.documents.elements.Formula at 0x2c385083a30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c385082b30>,\n",
       " <unstructured.documents.elements.Title at 0x2c385083b20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c385082bc0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b31c0>,\n",
       " <unstructured.documents.elements.Formula at 0x2c3f93046a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9307a90>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f93043d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9307a30>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f9305c90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9305960>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f9305a50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b2920>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9304790>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f9307640>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3853b2260>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9305ed0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9305540>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f9306500>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9306200>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f9304d90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9304d60>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f93077f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9305360>,\n",
       " <unstructured.documents.elements.Text at 0x2c3853b2080>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9305180>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f9304ac0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9306ad0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9306a10>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f93067d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9306920>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f9304760>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f9307490>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9306800>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f93074f0>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f9306890>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x2c3f9307d90>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b3bb0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b2380>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b0e80>,\n",
       " <unstructured.documents.elements.Table at 0x2c3f93058d0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b1840>,\n",
       " <unstructured.documents.elements.Text at 0x2c3853b2470>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9305fc0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f93044c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9305660>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f93064a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9304730>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f9305480>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f96c8580>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96cb4c0>,\n",
       " <unstructured.documents.elements.Text at 0x2c3853b1420>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f96c9540>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c9b70>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c87c0>,\n",
       " <unstructured.documents.elements.Table at 0x2c3f96c8af0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c8a60>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f96c80d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c9ba0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c8250>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c8ac0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c9330>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f96cac80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96cbfa0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b10c0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b1480>,\n",
       " <unstructured.documents.elements.Table at 0x2c3f96c8550>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x2c3f96c9de0>,\n",
       " <unstructured.documents.elements.Table at 0x2c3f96c9a50>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b28f0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3853b1570>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96ca980>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96cb700>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f96c9cf0>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x2c3f96c8c10>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f96c9270>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c9210>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f96cbd60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c9f90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c9f60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96cb3a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96c9ff0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f96ca170>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96ca2c0>,\n",
       " <unstructured.documents.elements.Text at 0x2c3853b21a0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f96caf20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96cb5b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96cb7c0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f96cb820>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f96cb9a0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f96ca440>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f96c8df0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f96ca860>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f96c9360>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823f9d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823ccd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823dfc0>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f823fd00>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823f760>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823c9d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823c8e0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823dcc0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823e410>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823ed10>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823c220>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f823f0d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89ff4c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89ff130>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89ff5e0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fee60>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fed70>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f89fc250>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fc9a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89ff9d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89ff730>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89ffdc0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89ff2b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89ffdf0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fd2d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fd690>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fdbd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fd1b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fd180>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fdd80>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f89fd150>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f89fd2a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fdc30>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fc880>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fd870>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fceb0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fc280>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89ff5b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fcfd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fd510>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fdff0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fd390>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fc4c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fc220>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f89ffb50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f89ffcd0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fcf40>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fc1f0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fc550>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89ffbb0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f89fc5e0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a08760>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a0a980>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a0bf70>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a09a80>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a095d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a097e0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a08a00>,\n",
       " <unstructured.documents.elements.Text at 0x2c38510c220>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a096f0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a08730>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a08640>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a09750>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f7a0aaa0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f86e87c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f86ebeb0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f86ebe50>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f86e8a30>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f86eb310>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f86eb610>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f86eaec0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f86ea6b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x2c3f86ebf10>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f86ea830>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f86e9ed0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f86eb370>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f86e90c0>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dfb20>,\n",
       " <unstructured.documents.elements.Image at 0x2c3f95df6d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95df5e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95df580>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95ded40>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95deec0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95de620>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f95dec20>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dec80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95df0d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95df400>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95deb00>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95deb30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95dc790>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95dcaf0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95de920>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95ddde0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95ddb10>,\n",
       " <unstructured.documents.elements.Footer at 0x2c3f95ddb40>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x2c3f95dc580>,\n",
       " <unstructured.documents.elements.Table at 0x2c3f95de140>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95de050>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95de650>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95dd750>,\n",
       " <unstructured.documents.elements.Title at 0x2c3f95df1c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x2c3f95de4d0>,\n",
       " <unstructured.documents.elements.Header at 0x2c3f95dde40>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "raw_pdf_elements2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table=[]\n",
    "for element in raw_pdf_elements2:\n",
    "  if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            Table.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - - /50.1 37.4 /60.5 44.7 - - Model B-1 QB-1 R-L B-1 Label Acc. Open REALM [20] 40.4 - / - 40.7 46.8 SotA - - 49.8* 49.9* 76.8 Book DPR [26] 41.5 57.9/ - 41.1 50.6 BART 15.1 19.7 38.2 41.6 64.0 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5',\n",
       " 'Task Input Model Generation MS- deﬁne middle ear BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. MARCO what currency BART The currency needed in Scotland is Pound sterling. needed in RAG-T Pound is the currency needed in Scotland. scotland RAG-S The currency needed in Scotland is the pound sterling. BART ?This state has the largest number of counties in the U.S. Jeopardy Washington RAG-T It’s the only U.S. state named for a U.S. president Question RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park Gener -ation The Divine Comedy BART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio RAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem RAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"',\n",
       " 'Factuality Speciﬁcity MSMARCO Jeopardy QGen BART better RAG better Both good Both poor No majority 7.1% 42.7% 11.7% 17.7% 20.8% 16.8% 37.4% 11.8% 6.9% 20.1% Gold BART RAG-Token RAG-Seq. 89.6% 70.7% 77.8% 83.5% 90.0% 32.4% 46.8% 53.8%',\n",
       " 'Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 Exact Match B-1 QB-1 R-L B-1 RAG-Token-BM25 RAG-Sequence-BM25 29.7 31.8 41.5 44.1 32.1 36.6 33.1 33.8 17.5 11.1 22.3 19.5 55.5 56.5 48.4 46.9 75.1 91.6 RAG-Token-Frozen RAG-Sequence-Frozen 37.8 41.2 50.1 52.1 37.1 41.8 51.1 52.6 16.7 11.8 21.7 19.6 55.9 56.7 49.4 47.3 72.9 89.4 RAG-Token RAG-Sequence 43.5 44.0 54.8 55.8 46.5 44.9 51.9 53.4 17.9 15.3 22.6 21.5 56.2 57.2 49.4 47.5 74.5 90.6',\n",
       " 'Task Train Development Test Natural Questions 79169 8758 3611 TriviaQA 78786 8838 11314 WebQuestions 3418 362 2033 CuratedTrec 635 134 635 Jeopardy Question Generation 97392 13714 26849 MS-MARCO 153726 12468 101093* FEVER-3-way 145450 10000 10000 FEVER-2-way 96966 6666 6666']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Text=[]\n",
    "for element in raw_pdf_elements2:\n",
    "  if \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "            Text.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Patrick Lewis't, Ethan Perez*,\",\n",
       " 'Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,',\n",
       " 'Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†',\n",
       " 'tRacebook AI Research; ‘University College London; *New York University;',\n",
       " 'plewis@fb.com',\n",
       " 'Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.',\n",
       " 'Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- edge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results,',\n",
       " 'Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents.',\n",
       " 'but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.',\n",
       " 'We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.',\n",
       " 'There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack- augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.',\n",
       " 'Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera- tion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1',\n",
       " 'We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized',\n",
       " '1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/',\n",
       " 'by θ that generates a current token based on a context of the previous i − 1 tokens y1:i−1, the original input x and a retrieved passage z.',\n",
       " 'To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pη and pθ components, as well as the training and decoding procedure.',\n",
       " 'RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,',\n",
       " 'RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:',\n",
       " 'Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.',\n",
       " 'The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:',\n",
       " 'where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.',\n",
       " 'The generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth.',\n",
       " 'We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj, yj), we',\n",
       " 'minimize the negative marginal log-likelihood of each target, y)',\n",
       " 'j − log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.',\n",
       " 'At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x).',\n",
       " 'RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: pj (yi|z, yii—1) = De zetop-k(p(-l2)) Pn (Zi|@)po(Yil@, Zi, Y14—-1) To decode, we can plug Po(yi |x, y1i—1) into a standard beam decoder.',\n",
       " 'RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x, zi) ≈ 0 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as “Fast Decoding.”',\n",
       " 'We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k ∈ {5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task.',\n",
       " 'Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.',\n",
       " 'RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat',\n",
       " 'MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.',\n",
       " 'To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.',\n",
       " 'We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—quuestion A is better, question B is better, both are good, or neither is good.',\n",
       " 'FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.',\n",
       " 'Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross- encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.',\n",
       " 'There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading',\n",
       " 'to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%.',\n",
       " 'As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5).',\n",
       " 'Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more speciﬁc by a large margin. Table 3 shows typical generations from each model.',\n",
       " 'Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens. This observation suggests that the generator can complete the titles without depending on speciﬁc documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.',\n",
       " 'Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.',\n",
       " 'Figure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem- ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".',\n",
       " 'Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.',\n",
       " 'For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.',\n",
       " 'Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding [33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing any diversity-promoting decoding.',\n",
       " 'Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.',\n",
       " 'We compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.',\n",
       " 'Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed',\n",
       " 'Table 4: Human assessments for the Jeopardy Question Generation Task.',\n",
       " 'between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.',\n",
       " 'Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe signiﬁcant differences in performance between them. We have the ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.',\n",
       " 'Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.',\n",
       " 'General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench- marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed architecture, by learning a retrieval module to augment pre-trained, generative language models.',\n",
       " 'Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be ﬁne-tuned for strong performance on a variety of tasks.',\n",
       " 'Memory-based Architectures Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].',\n",
       " 'Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a ﬁnal output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.',\n",
       " 'In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual and speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.',\n",
       " 'This work offers several positive societal beneﬁts over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.',\n",
       " 'With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against misleading content and automated spam/phishing.',\n",
       " 'The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.',\n",
       " 'for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/ anthology/P19-1612.',\n",
       " 'approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_ 2016_paper9.pdf.',\n",
       " 'For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.',\n",
       " 'Figure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\".',\n",
       " 'Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instruc- tions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.',\n",
       " 'We train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision ﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search with FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼ 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers [66]3, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/ blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/',\n",
       " '2https://github.com/pytorch/fairseq 3https://github.com/huggingface/transformers',\n",
       " 'For open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to ﬁnd matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur in top 1000 documents for the query.',\n",
       " 'CuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres- sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.',\n",
       " 'TriviaQA Evaluation setups The open-domain QA community customarily uses public develop- ment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading compehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more conventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being simpler to answer from Wikipedia.',\n",
       " 'For FEVER classiﬁcation, we follow the practice from [32], and ﬁrst re-generate the claim, and then classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The ﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.',\n",
       " 'We experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document mechanisms may not be necessary for RAG.',\n",
       " 'Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable',\n",
       " 'parameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non- parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consists of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating point precision to manage memory and disk footprints.',\n",
       " 'In preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would “collapse” and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.',\n",
       " 'The number of training, development and test datapoints in each of our datasets is shown in Table 7.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image=[]\n",
    "for element in raw_pdf_elements2:\n",
    "  if \"unstructured.documents.elements.Image\" in str(type(element)):\n",
    "            Image.append(str(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@€--- ee ee ee ee ee ee eee ee ee eee The middle ear includes End-to-End Backprop through q and pe the tympanic cavity and the three ossicles. (y) Define \"middle ear\" (x) Question Answering: Question Query Retriever py Document Generator pg ‘ower Generation index (Non-Parametric) (Parametric) d(z) supports (y) Barack Obama was born in Hawaii. (x) q(x) Fact Verification: Fact Query Fact Verification: Label Generation The Divine This 14th century work Comedy (x) is divided into 3 Jeopardy Question Generation: Answer Query sections: \"Inferno\", \"purgatorio\" & \"Paradiso\" @) Question Generation',\n",
       " 'Document 1: his works are considered classics of American Doc 1 | | literature ... His wartime experiences formed the basis for his novel poe 2 | | “A Farewell to Arms” (1929) ... Doc 3 Document 2: ... artists of the 1920s “Lost Generation” expatriate Doe 4 community. His debut novel, \"The Sun Also Rises”, was published °° in 1926. Doc 5 & , es, ee £ tg ° es & ss . TESS SK',\n",
       " '| % 80 De ranananaEnEEREnEEEREE EERE g6 g / 2 fr = 70 Z| / 3 — RAG TORRE 2 nf |! g == RAG-Tok B-1 Eom EB 60 ==- RAG-Seq RL Fs F a 2, == RAGSeq B41 Q 50 > 50 ZO — reactor | & 3 soft === RAGSeq | Z 40 24s rr rr a a) rr a a rr a K Retrieved Docs K Retrieved Docs K Retrieved Docs',\n",
       " 'View full instructions Which sentence is more factually true? View tool guide Select an option Subject: Hemingway a Note: Some questions are Sentence Ais more 1 control questions. We require Sentence A : \"The Sun Also Rises\" is a novel by this author of \"A true good accuracy on our control Farewell to Arms\" Sentence Bismore 2 questions to accept true responses. Sentence B : This author of \"The Sun Also Rises\" was born in Both sentences are 8 Havana, Cuba, the son of Spanish immigrants ‘true Indicate which one of the P 3 following sentences is more Both sentences are factually true with respect to completely untrue the subject. Using the internet to check whether the sentences are true is encouraged.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw table elements. \\\n",
    "    Give a concise summary of the table that is well optimized for retrieval. Table:{element} \"\"\"\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_TOKEN = os.getenv('OPENAI_API_KEY')  # Fetch API key from system environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table_summaries = summarize_chain.batch(Table, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The table presents the performance of various models on a task, measured by accuracy. The models include Closed Book T5-11B, T5-11B+SSM, Model B-1, QB-1, R-L B-1, REALM, SotA, Book DPR, BART, RAG-Token, RAG-Seq, and RAG-Tok. The performance is measured in different settings, including Open, Label, and Acc. The highest accuracy is achieved by RAG-Seq with 89.5, followed by BART with 81.1. The lowest accuracy is shown by RAG-Tok with 17.3.',\n",
       " 'The table provides information on various topics generated by different models. It includes definitions of the middle ear, the currency used in Scotland, a trivia about the US state with the most counties, and a brief description of Dante\\'s \"The Divine Comedy\". The middle ear is described as the part of the ear internal to the eardrum, including the tympanic cavity and the three ossicles. The currency in Scotland is identified as the Pound Sterling. The state with the most counties in the US is hinted to be the only one named after a US president and home to Mount Rainier National Park. \"The Divine Comedy\" is described as a 14th-century epic poem by Dante, divided into three parts: Inferno, Purgatorio, and Paradiso.',\n",
       " 'The table presents a comparison of different models (MSMARCO, Jeopardy, QGen, BART, RAG, Gold, BART, RAG-Token, RAG-Seq) based on their performance in terms of factuality, specificity, and whether they are better, good, poor, or have no majority. The highest percentage for factuality and specificity is achieved by Gold at 89.6% and BART at 90.0% respectively. The model with the highest percentage for being better is MSMARCO at 42.7%, while RAG-Seq has the highest percentage for being good at 53.8%. The model with the highest percentage for being poor is BART at 20.8%. The model with the highest percentage for having no majority is MSMARCO at 20.1%.',\n",
       " 'The table presents the performance metrics of different models (NQ, TQA, WQ, CT, Jeopardy-QGen, MSMarco, FVR-3, Exact Match, B-1, QB-1, R-L, B-1) using various methods (RAG-Token-BM25, RAG-Sequence-BM25, RAG-Token-Frozen, RAG-Sequence-Frozen, RAG-Token, RAG-Sequence). The metrics are presumably percentages, with higher values indicating better performance. The RAG-Token and RAG-Sequence methods generally perform better than the others.',\n",
       " 'The table presents the distribution of data across training, development, and testing sets for various tasks including Natural Questions, TriviaQA, WebQuestions, CuratedTrec, Jeopardy Question Generation, MS-MARCO, FEVER-3-way, and FEVER-2-way. The data ranges from as low as 635 for CuratedTrec to as high as 153726 for MS-MARCO in the training sets. The development and test sets also vary similarly across tasks.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval.text: {element} \"\"\"\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty summaries\n",
    "\n",
    "text_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries = summarize_chain.batch(Text, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The text mentions two individuals, Patrick Lewis't and Ethan Perez*, but does not provide any additional information or context about them.\",\n",
       " 'The text appears to list the names of several individuals, possibly authors or researchers, including Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, and Heinrich Küttler.',\n",
       " 'The text lists the names of five individuals: Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.',\n",
       " 'Summary: The text refers to three institutions: Facebook AI Research, University College London, and New York University.',\n",
       " 'Summary: Email address of a person named P. Lewis at Facebook.',\n",
       " 'The text discusses the limitations of large pre-trained language models in accessing and manipulating knowledge, especially in knowledge-intensive tasks. It introduces Retrieval-Augmented Generation (RAG) models, which combine pre-trained parametric and non-parametric memory for language generation. These models use a pre-trained seq2seq model as parametric memory and a dense vector index of Wikipedia as non-parametric memory. Two RAG formulations are compared, one that uses the same retrieved passages for the entire generated sequence, and another that uses different passages per token. The RAG models are evaluated on various NLP tasks and set new standards on three open domain QA tasks, outperforming other models. They also generate more specific, diverse, and factual language for language generation tasks.',\n",
       " 'Pre-trained neural language models can learn in-depth knowledge from data without external memory, acting as an implicit knowledge base. However, they have limitations such as difficulty in expanding or revising memory, providing insight into predictions, and may produce \"hallucinations\". Hybrid models, combining parametric memory with retrieval-based memories, can address these issues as knowledge can be directly revised, expanded, and interpreted. Models like REALM and ORQA, which combine masked language models with a differentiable retriever, have shown promising results.',\n",
       " 'The text describes an approach that combines a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator), fine-tuned end-to-end. A query x uses Maximum Inner Product Search (MIPS) to find the top-K documents zi. The final prediction y treats z as a latent variable and marginalizes over seq2seq predictions given different documents.',\n",
       " 'The text discusses the exploration of open-domain extractive question answering and the integration of hybrid parametric and non-parametric memory into sequence-to-sequence (seq2seq) models, a fundamental component of NLP.',\n",
       " 'The text discusses the development of retrieval-augmented generation (RAG) models, which combine pre-trained, parametric-memory generation models with a non-parametric memory. The parametric memory is a pre-trained seq2seq transformer, while the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. The model is trained end-to-end, with the retriever providing latent documents based on the input, and the seq2seq model generating the output based on these latent documents and the input. The model can be fine-tuned for any seq2seq task, with both the generator and retriever being jointly learned.',\n",
       " 'The text discusses previous work on architectures designed to enhance systems with non-parametric memory, specifically for certain tasks. Examples include memory networks, stack-augmented networks, and memory layers. The text contrasts this with a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge, eliminating the need for additional training.',\n",
       " \"The text discusses the advantages of combining parametric and non-parametric memory for knowledge-intensive tasks. The RAG models developed using this approach have achieved top results on Natural Questions, WebQuestions, and CuratedTrec, and outperformed other methods on TriviaQA. Despite these tasks being extractive, unconstrained generation has proven more effective. The models have also shown to generate more factual, specific, and diverse responses in MS-MARCO and Jeopardy question generation compared to a BART baseline. For FEVER fact verification, the models achieved results close to top pipeline models. The non-parametric memory can be updated to keep the models' knowledge current.\",\n",
       " 'The text discusses RAG models that utilize an input sequence to retrieve text documents and use them as context for generating a target sequence. The models use two components: a retriever that returns distributions over text passages based on a query, and a generator that is parameterized.',\n",
       " 'The code for running experiments with RAG is open-sourced and part of the HuggingFace Transformers Library, available on GitHub. An interactive demo of RAG models is also accessible on the HuggingFace website.',\n",
       " 'The text discusses a function θ that generates a current token. This function is influenced by the context of previous tokens, the original input, and a retrieved passage.',\n",
       " 'The text discusses the training of a retriever and generator using retrieved documents as latent variables. Two models, RAG-Sequence and RAG-Token, are proposed which use these documents differently to predict target tokens. The RAG-Sequence uses the same document for each token prediction, while the RAG-Token can use different documents for each token. The text also mentions the introduction of pη and pθ components and the training and decoding procedure.',\n",
       " 'The RAG-Sequence model generates a complete sequence using a single retrieved document as a latent variable. It calculates the seq2seq probability through a top-K approximation, where the top K documents are retrieved and the generator produces the output sequence probability for each document.',\n",
       " 'The RAG-Token model allows for the selection of different latent documents for each target token, enabling the generator to use content from multiple documents when creating an answer. The top K documents are retrieved and the generator produces a distribution for the next output token for each document, before marginalizing and repeating the process with the subsequent output token.',\n",
       " 'RAG can be utilized for sequence classification tasks by treating the target class as a single-length target sequence, making RAG-Sequence and RAG-Token identical in this context.',\n",
       " 'The retrieval component pη(z|x) utilizes the bi-encoder architecture of Dense Passage Retrieval (DPR) as outlined in reference [26].',\n",
       " 'The text discusses a document retrieval process using a BERTBASE document encoder to create a dense representation of a document and a query encoder for query representation. The top-k documents with the highest prior probability are calculated, which is a Maximum Inner Product Search (MIPS) problem. A pre-trained bi-encoder from DPR is used to initialize the retriever and build the document index. The retriever is trained to retrieve documents containing answers to TriviaQA and Natural Questions. The document index is referred to as the non-parametric memory.',\n",
       " 'The text discusses the use of BART-large, a pre-trained seq2seq transformer with 400M parameters, as a generator component in a model. The input is combined with retrieved content by concatenation for generation from BART. BART, pre-trained using a denoising objective and various noising functions, has achieved top results in diverse generation tasks, outperforming similar-sized T5 models. The BART generator parameters are referred to as the parametric memory.',\n",
       " 'The text discusses a method of jointly training the retriever and generator components without direct supervision on document retrieval. This process involves using a fine-tuning training corpus of input/output pairs.',\n",
       " 'The text discusses the goal of reducing the negative marginal log-likelihood for each target variable, denoted as y.',\n",
       " \"The text discusses the use of stochastic gradient descent with Adam for the equation j − log p(yj|xj). It mentions the costliness of updating the document encoder BERTd during training due to the need for periodic document index updates, as done by REALM during pre-training. However, it suggests that this step isn't necessary for strong performance. Instead, only the query encoder BERTq and the BART generator are fine-tuned, keeping the document encoder and index fixed.\",\n",
       " 'The text discusses the different methods RAG-Sequence and RAG-Token use to approximate arg maxy p(y|x) during testing.',\n",
       " 'The RAG-Token model is an autoregressive seq2seq generator with a specific transition probability formula. It can be decoded by integrating it into a standard beam decoder.',\n",
       " 'The RAG-Sequence uses a unique approach to calculate the likelihood p(y|x), which cannot be solved with a single beam search. Instead, a beam search is run for each document, scoring each hypothesis. This results in a set of hypotheses, some of which may not appear in all document beams. To estimate the probability of a hypothesis, an additional forward pass is run for each document where the hypothesis does not appear in the beam. The decoding procedure, referred to as \"Thorough Decoding\", can be time-consuming for longer output sequences. To increase efficiency, an approximation can be made to avoid additional forward passes, referred to as \"Fast Decoding\".',\n",
       " 'The text discusses an experiment with RAG across various knowledge-intensive tasks, using a single Wikipedia dump as a non-parametric knowledge source. The December 2018 dump is used, with each article divided into 100-word chunks, resulting in 21M documents. These documents are embedded using a document encoder and a MIPS index is built for fast retrieval. During training, the top k documents are retrieved for each query, with k being either 5 or 10. The value of k for testing is determined using dev data.',\n",
       " 'The text discusses open-domain question answering (QA), comparing Retrieval-Augmented Generation (RAG) to extractive QA and Closed-Book QA approaches. RAG is trained by minimizing the negative log-likelihood of answers, while extractive QA relies on non-parametric knowledge and Closed-Book QA on parametric knowledge. Four open-domain QA datasets are used: Natural Questions (NQ), TriviaQA (TQA), WebQuestions (WQ), and CuratedTrec (CT). The smaller CT and WQ models are initialized with the NQ RAG model. The models are evaluated using the same train/dev/test splits as previous studies and Exact Match (EM) scores. TQA is also evaluated on the TQA Wiki test set for comparison with T5.',\n",
       " \"The text discusses the use of RAG models for free-form, abstractive text generation, beyond simple extractive QA. It mentions a test of RAG's natural language generation capabilities using the MSMARCO NLG task v2.1, which includes questions, ten gold passages, and a full sentence answer. The test only uses the questions and answers, not the supplied passages.\",\n",
       " 'MSMARCO is an open-domain abstractive QA task with certain questions that require access to gold passages for accurate answers. Performance may decrease without these passages. Some questions cannot be answered using only Wikipedia, but RAG can use parametric knowledge to generate reasonable responses.',\n",
       " \"The text discusses the evaluation of RAG's generation abilities in an open-domain question generation setting. Instead of using standard open-domain QA tasks, the study proposes a more challenging task of generating Jeopardy questions. Jeopardy questions require guessing an entity based on a fact about it, making it a knowledge-intensive generation task.\",\n",
       " 'The text discusses the use of SearchQA splits for training, development, and testing. A BART model is trained for comparison and evaluated using the SQuAD-tuned Q-BLEU-1 metric, a variant of BLEU that prioritizes entity matching. Two human evaluations are conducted to assess factuality and specificity of generated content. Factuality is defined as the ability to corroborate a statement with trusted sources, while specificity refers to the high mutual dependence between input and output. The evaluation process involves a pairwise comparative evaluation where evaluators are shown an answer and two generated questions, one from BART and one from RAG, and asked to choose the better question.',\n",
       " \"FEVER is a task that involves classifying the validity of a claim based on information from Wikipedia. It requires retrieving relevant evidence and reasoning over it to determine if the claim is true, false, or unverifiable. FEVER is a retrieval problem with an entailment reasoning task, and serves as a testbed for exploring the RAG models' classification capabilities. The FEVER class labels are mapped to single output tokens and trained with claim-class pairs. Unlike other approaches, this method does not use supervision on retrieved evidence, making it applicable to a wider range of tasks. Two variants are explored: a standard 3-way classification task and a 2-way task. Label accuracy is reported in both cases.\",\n",
       " 'Table 1 presents the performance of RAG and other state-of-the-art models on four open-domain QA tasks. RAG sets a new standard, particularly on the T5-comparable split for TQA, by combining the benefits of both \"closed-book\" and \"open-book\" approaches. Unlike other models, RAG achieves strong results without the need for costly, specialized pre-training. RAG\\'s retriever is initialized using DPR\\'s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG outperforms the DPR QA system, proving that neither a re-ranker nor an extractive reader is necessary for top-tier performance.',\n",
       " 'The text discusses the benefits of generating answers instead of extracting them. It highlights that documents with indirect clues can contribute to generating the correct answer, a feature not possible with standard extractive methods.',\n",
       " \"The text discusses the effectiveness of RAG (Retrieval-Augmented Generation) in document marginalization. It highlights RAG's ability to generate correct answers even when the answer is not present in any retrieved document, achieving 11.8% accuracy in such cases for NQ, compared to an extractive model's 0% score.\",\n",
       " 'The text discusses the superior performance of RAG-Sequence over BART on Open MS-MARCO NLG, with a lead of 2.6 Bleu and Rouge-L points. Despite limitations such as the inability to answer all questions from Wikipedia alone, RAG models approach state-of-the-art performance. They generate factually correct text more frequently and exhibit less hallucination than BART. Additionally, RAG models produce more diverse generations than BART.',\n",
       " 'RAG-Token outperforms RAG-Sequence and BART in Jeopardy question generation according to Table 2. Human evaluation results show that RAG was more factual than BART in 42.7% of cases, while BART was more factual in only 7.1% of cases. RAG and BART were equally factual in 17% of cases. Evaluators found RAG generations to be more specific. Table 3 provides examples of typical generations from each model.',\n",
       " 'The RAG-Token model in Jeopardy questions can generate responses that combine content from multiple documents. The model\\'s parametric knowledge is sufficient to complete book titles without relying on specific documents. This is demonstrated by the BART-only baseline, which can complete the generation of titles like \"The Sun Also Rises\" and \"A Farewell to Arms\". This shows the collaboration of parametric and non-parametric memories, where the non-parametric component guides the generation using specific knowledge stored in the parametric memory.',\n",
       " \"Table 2 presents the performance of RAG on FEVER in 3-way classification. Despite its lack of domain-specific architectures and intermediate retrieval supervision, RAG's scores are within 4.3% of complex, state-of-the-art models.\",\n",
       " 'The text describes Figure 2, which illustrates the RAG-Token document posterior for each generated token for the input \"Hemingway\" in a Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \"A Farewell to Arms\" and for document 2 when generating \"The Sun Also Rises\".',\n",
       " \"Table 3 showcases examples from generation tasks, demonstrating that RAG models produce more specific and factually accurate responses. The table uses '?' to indicate factually incorrect responses and '*' for partially correct responses.\",\n",
       " \"The text discusses a comparison of a 2-way classification model trained by Thorne and Vlachos using RoBERTa, against RAG. Despite only being supplied with the claim and retrieving its own evidence, RAG's accuracy is within 2.7% of the RoBERTa model. An analysis of the documents retrieved by RAG shows a 71% match with gold evidence articles in the top retrieved document and a 90% match in the top 10 retrieved articles.\",\n",
       " \"The text discusses the diversity in question generation of RAG models compared to BART. It states that RAG models are more factual and specific. The diversity is measured by the ratio of distinct ngrams to total ngrams. RAG-Sequence's generations are found to be more diverse than RAG-Token's, and both are significantly more diverse than BART, without requiring any diversity-promoting decoding.\",\n",
       " 'The text discusses the importance of the retrieval mechanism in RAG (Retrieval-Augmented Generation). It mentions an experiment where the retriever is frozen during training to assess its effectiveness. The results, as per Table 6, indicate that learned retrieval enhances outcomes for all tasks.',\n",
       " \"The text compares RAG's dense retriever to a BM25 retriever, replacing RAG's retriever with a fixed BM25 system. The results show that BM25 performs best for FEVER, likely due to its entity-centric claims. However, differentiable retrieval improves results on all other tasks, particularly in Open-Domain QA.\",\n",
       " 'The text discusses the advantage of non-parametric memory models like RAG over parametric-only models like T5 or BART, highlighting their ability to easily update knowledge at test time. It explains a demonstration where an index was built using the DrQA Wikipedia dump from December 2016 and compared with a newer index from December 2018. The demonstration involved a list of 82 world leaders who had changed.',\n",
       " 'Summary: The table, titled \"Human assessments for the Jeopardy Question Generation Task,\" presents the evaluation results of human assessments conducted for the Jeopardy question generation task.',\n",
       " \"The text discusses the accuracy of the NQ RAG model in identifying world leaders based on the year. The model correctly identifies 70% of 2016 leaders using the 2016 index and 68% of 2018 leaders using the 2018 index. However, accuracy drops significantly when mismatched indices are used. The text suggests that updating RAG's world knowledge can be achieved by replacing its non-parametric memory.\",\n",
       " 'The text discusses the impact of retrieving more documents on the performance of models trained with either 5 or 10 latent documents. It is found that increasing the number of retrieved documents at test time improves Open-domain QA results for RAG-Sequence, with performance peaking for RAG-Token at 10 documents. However, retrieving more documents also increases Rouge-L for RAG-Token at the cost of Bleu-1, with a less significant effect for RAG-Sequence.',\n",
       " 'The text discusses the effectiveness of retrieval in enhancing performance in various Natural Language Processing (NLP) tasks such as open-domain question answering, fact checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. The work aims to unify these successes by demonstrating that a single retrieval-based architecture can deliver strong performance across multiple tasks.',\n",
       " 'The text discusses the evolution and success of general-purpose architectures for Natural Language Processing (NLP) tasks. It highlights the effectiveness of single, pre-trained language models in achieving strong performance on various tasks, as demonstrated by GPT-2, BART, and T5. The text also mentions the use of bi-directional attention to improve performance. The authors aim to further expand the potential tasks by incorporating a retrieval module into pre-trained, generative language models.',\n",
       " 'The text discusses the significant progress in the field of information retrieval, particularly with the use of pre-trained, neural language models. It mentions various optimization techniques for the retrieval module, including search, reinforcement learning, and a latent variable approach, which are often used for specific tasks like question answering. The text highlights that while previous work has focused on optimizing performance for individual tasks, the authors demonstrate that a single retrieval-based architecture can be fine-tuned for strong performance across multiple tasks.',\n",
       " \"The document discusses memory-based architectures for neural networks, comparing different methods of retrieving and using information. The authors' approach uses raw text in the memory, making it human-readable and writable, and allowing for dynamic updates by editing the document index. This method has been used in knowledge-intensive dialog and offers a form of interpretability. Other methods discussed include retrieving trained embeddings for each entity in the input and improving dialog models' ability to generate factual text by attending over fact embeddings.\",\n",
       " \"The text discusses Retrieve-and-Edit approaches, which involve retrieving a similar training input-output pair for a given input and editing it to provide a final output. These methods have been successful in Machine Translation and Semantic Parsing. The author's approach differs by focusing less on editing a retrieved item and more on aggregating content from multiple retrieved pieces, learning latent retrieval, and retrieving evidence documents instead of related training pairs. The text suggests that RAG techniques could be effective in these settings and may be a promising area for future research.\",\n",
       " 'This work introduces hybrid generation models with parametric and non-parametric memory, known as RAG models, which achieve top results in open-domain QA. RAG models are preferred over purely parametric BART due to their factual and specific nature. The study validates the effectiveness of the learned retrieval component and demonstrates how the retrieval index can be updated without retraining. Future research may explore joint pre-training of the two components. The study suggests potential for these models in various NLP tasks and further research on the interaction and combination of parametric and non-parametric memories.',\n",
       " 'This work provides societal benefits by being more factually accurate and controllable due to its grounding in real knowledge, specifically Wikipedia. It reduces \"hallucinations\" in generations and increases interpretability. The Retriever-Augmented Generation (RAG) model can be used in various scenarios, such as answering open-domain questions on medical topics or improving job efficiency.',\n",
       " 'The text discusses potential drawbacks of using external knowledge sources like Wikipedia, which may not be entirely factual or unbiased. It also raises concerns about the use of RAG as a language model, similar to GPT-2, which could be used to generate misleading content, impersonate others, or automate spam/phishing content. The text suggests that advanced language models could automate various jobs in the future. To counter these risks, the use of AI systems to combat misleading content and automated spam/phishing is proposed.',\n",
       " 'The authors express gratitude to the reviewers for their feedback, HuggingFace for assistance in open-sourcing RAG models, and Kyunghyun Cho and Sewon Min for their advice. EP acknowledges support from the NSF Graduate Research Fellowship, while PL is supported by the FAIR PhD program.',\n",
       " 'This text refers to a document or study related to Computational Linguistics, published in July 2019 in Florence, Italy. The document is associated with the Association for Computational Linguistics and can be accessed through the provided URL. The DOI for the document is 10.18653/v1/P19-1612.',\n",
       " 'The text refers to the 2016 Approaches co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016) held in Barcelona, Spain. The proceedings of the conference are documented in volume 1773 of CEUR Workshop Proceedings and are available online.',\n",
       " \"The text discusses test results for Open-domain QA, Open-MSMarco, and Jeopardy question generation using RAG-Token and RAG-Sequence models. For Open-domain QA, 15 documents were retrieved for RAG-Token and 50 for RAG-Sequence, using Thorough Decoding. Greedy decoding was used as beam search didn't improve results. For Open-MSMarco and Jeopardy, ten documents were retrieved for both models, with a BART-large model used as a baseline. A beam size of four was used, and Fast Decoding was applied for RAG-Sequence models as Thorough Decoding didn't enhance performance.\",\n",
       " 'The text describes Figure 4, which is an annotation interface used for human evaluation of factuality. Detailed instructions and examples are provided in a pop-out when \"view tool guide\" is clicked.',\n",
       " \"Figure 4 illustrates the user interface for human evaluation, designed to prevent screen position bias. Annotators were guided to research topics online and provided with comprehensive instructions and examples. The inclusion of gold sentences helped assess annotator accuracy, leading to the removal of two annotators' work due to poor performance.\",\n",
       " 'The text discusses the training of RAG models and BART baselines using Fairseq, with mixed precision floating point arithmetic and distribution across 8, 32GB NVIDIA V100 GPUs. Maximum Inner Product Search with FAISS is used for fast CPU operations, requiring around 100 GB of CPU memory for all of Wikipedia. The code has been ported to HuggingFace Transformers, which is a cleaner, easier to use implementation that achieves equivalent performance. The document index is compressed using FAISS’s tools, reducing the CPU memory requirement to 36GB. Links are provided for scripts to run experiments with RAG and an interactive demo of a RAG model.',\n",
       " \"The text provides links to two GitHub repositories: one for PyTorch's Fairseq, a general-purpose sequence-to-sequence library, and the other for Hugging Face's Transformers, a state-of-the-art Natural Language Processing library.\",\n",
       " 'The text discusses the use of multiple answer annotations in open-domain QA for training extractive models. The RAG model also utilizes multiple annotation examples from Natural Questions and WebQuestions, training with each question-answer pair separately, which slightly improves accuracy. For TriviaQA, where many valid answers exist, unsuitable training targets like emojis or spelling variants are filtered out. Answer candidates not appearing in the top 1000 documents for the query are also excluded.',\n",
       " 'The text discusses the preprocessing method for CuratedTrec, which involves retrieving the top 1000 documents for each query and using the most frequently matched regex pattern as the supervision target. If no matches are found, a heuristic is used to generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.',\n",
       " 'The open-domain QA community typically uses public development datasets as test datasets due to restrictions on QA datasets. The TriviaQA test dataset used is the public TriviaQA Web Development split, consistent with common practice. However, some researchers use the official TriviaQA Wikipedia test set for comparison purposes. Performance tends to be higher on the official Wiki test set, likely due to simpler questions that can be answered from Wikipedia. Both test sets are reported on for fair comparison.',\n",
       " 'The text discusses the process of FEVER classification, which involves regenerating a claim and classifying it using the final hidden state representation. The classification results in probabilities for three categories: \"Supported\", \"Refuted\", or \"Not Enough Info\". The text also mentions a sub-task of extracting supporting evidence from Wikipedia, which is complicated due to the use of a different Wikipedia dump. The authors plan to address this issue in future work.',\n",
       " 'The text discusses an experiment with adding a \"Null document\" mechanism to RAG, similar to REALM, to model situations where no useful information can be retrieved. Three methods were tested: a document embedding for the null document, a static learnt bias term, and a neural network to predict the logit. However, none of these methods improved performance. In the Open MS-MARCO context, the model learns to retrieve a specific set of documents for less beneficial questions, suggesting that null document mechanisms may not be necessary for RAG.',\n",
       " 'The text discusses RAG models which include trainable parameters for the BERT-base query and document encoder of DPR, each having 110M parameters. The document encoder is not trained by the authors. The models also contain 406M trainable parameters from BART-large, totaling 626M trainable parameters.',\n",
       " 'The best performing \"closed-book\" open-domain QA model is T5-11B with 11 billion trainable parameters. T5-large, with 770M parameters, scores 28.9 EM on Natural Questions, significantly lower than the 44.5 achieved by RAG-Sequence, suggesting that hybrid models require fewer parameters for strong performance. The non-parametric memory index, comprising 21M 728-dimensional vectors or 15.3B values, can be stored at 8-bit floating point precision for efficient memory and disk management.',\n",
       " 'In initial tests, it was found that for certain tasks like story generation, the retrieval component would consistently retrieve the same documents irrespective of the input, causing the generator to disregard the documents and the RAG model to perform similarly to BART. This could be due to a lesser need for factual knowledge in some tasks or longer target sequences leading to less informative gradients for the retriever. Perez et al. also observed spurious retrieval results when optimizing a retrieval component for better performance on subsequent tasks.',\n",
       " 'Table 7 displays the quantity of training, development, and test datapoints in each dataset.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import base64\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4-turbo\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "\n",
    "    return img_base64_list, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpath=\"extracted_data2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image features a screenshot from a quiz-like interface. The screen is divided into three main parts. On the left, there are buttons for \"View full instructions\" and \"View tool guide\". A notice beneath these suggests some questions are control questions leading to a necessity for accuracy. The middle section includes a question, \"Which sentence is more factually true?\", regarding the subject \"Hemingway\". Two sentences are listed for evaluation: Sentence A states \"The Sun Also Rises\" is a novel by Hemingway who also wrote \"A Farewell to Arms\". Sentence B claims Hemingway, the author of \"The Sun Also Rises\", was born in Havana, Cuba to Spanish immigrants. The right section of the image provides options to select which statement is more factually true, numbered from 1 to 4.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The image features a screenshot from a quiz-like interface. The screen is divided into three main parts. On the left, there are buttons for \"View full instructions\" and \"View tool guide\". A notice beneath these suggests some questions are control questions leading to a necessity for accuracy. The middle section includes a question, \"Which sentence is more factually true?\", regarding the subject \"Hemingway\". Two sentences are listed for evaluation: Sentence A states \"The Sun Also Rises\" is a novel by Hemingway who also wrote \"A Farewell to Arms\". Sentence B claims Hemingway, the author of \"The Sun Also Rises\", was born in Havana, Cuba to Spanish immigrants. The right section of the image provides options to select which statement is more factually true, numbered from 1 to 4.',\n",
       " 'This image is a diagram representing an end-to-end machine learning model designed for natural language processing applications. It illustrates the integration of a query encoder, a non-parametric retriever, a document index, and a parametric generator for tasks such as question answering, fact verification, and question generation. The diagram shows the flow of information from input queries through encoding, retrieval of documents, and output generation, with the process allowing for backpropagation to refine model responses. Visual elements include labeled arrows for data flow, multiple blocks representing different components of the model, and text boxes explaining specific examples of input and output.',\n",
       " 'This image consists of two parts. The top portion has text describing two documents related to American literature and notable novels from the 1920s, referencing authors from the \"Lost Generation\" and novels \"A Farewell to Arms\" and \"The Sun Also Rises.\" The bottom portion is a heatmap visualization displaying term-document matrix of keywords such as \"novel,\" \"author,\" and specific book titles across five different documents. The heatmap uses shades of blue to indicate the frequency of each term\\'s occurrence in the documents.',\n",
       " 'This image consists of three separate line graphs, each plotting different metrics based on the number of retrieved documents (K Retrieved Docs).\\n\\n1. **Left Graph:** Displays the lines for \"NQ-Exact Match\" performance comparing RAG-Tok and RAG-Seq approaches, emphasizing performance stability beyond 10 documents retrieved.\\n   \\n2. **Middle Graph:** Illustrates \"NQ Answer Recall\" for RAG-Tok, RAG-Seq, Fixed DPR, and BM25 models, showing an increase in recall up to about 20 documents before plateauing, particularly highlighting better performance of sequence-based methods.\\n   \\n3. **Right Graph:** Depicts the \"Bleu/Rouge Score\" for RAG-Tok and RAG-Seq under different recall levels (RL and B-1). The graph signifies the relatively consistent performance across different methods and preserved efficiency with increased document retrieval.\\n   \\nThese graphs are typically useful in machine learning or information retrieval contexts, where such metrics are crucial for evaluating the performance of various retrieval algorithms.',\n",
       " 'This image contains a table listing different tasks and the number of data entries for training, development, and testing phases. Tasks include \"Natural Questions,\" \"TriviaQA,\" \"WebQuestions,\" \"CuratedTrec,\" \"Jeopardy Question Generation,\" \"MS-MARCO,\" \"FEVER-3-way,\" and \"FEVER-2-way,\" with varying numbers of data entries in each category. The \"MS-MARCO\" test section is marked with an asterisk, suggesting an anomaly or note about this particular data point.',\n",
       " 'This image is a comparison table of various model performances across different datasets and metrics related to natural language processing tasks. The table is split into two sections. The left section is titled \"Model\" and lists various models under categories \"Closed Book\" and \"Open Book,\" showing performance scores on NQ, TQA, WQ, and CT metrics. Models include T5-11B, T5-11B+SSM, REALM, DPR, RAG-Token, and RAG-Seq, with associated references and scores in green. The right section lists scores for Jeopardy and MSMARCO datasets, along with FVR3 and FVR2 metrics. Models in this section include SotA, BART, RAG-Tok, and RAG-Seq, with some scores highlighted in green indicating state-of-the-art or notable performance.',\n",
       " 'The image contains a table comparing the output of different models (BART, RAG-T, RAG-S) answering various questions under different tasks like MS-MARCO, Jeopardy Question Generation, and The Divine Comedy explanation. Each row displays a task, an input query, the model used, and the responses generated by each model for the query.',\n",
       " 'This image presents a table comparing the performance of different algorithm models (BART, RAG-Token, and RAG-Seq) across multiple metrics, including factuality, specificity and performance on MSMARCO and Jeopardy Question Generation (QGen) tasks. It provides numerical percentage values to show how often each model performed better, both models performed poorly, or when there was no majority in performance. The table is divided into two sections, one for factuality and specificity comparison, and another for Gold standard performances on MSMARCO and Jeopardy QGen tasks.',\n",
       " 'The image is a table comparing the performance of various RAG (Retrieval-Augmented Generation) model configurations across multiple tasks and metrics. The models evaluated include RAG-Token-BM25, RAG-Sequence-BM25, RAG-Token-Frozen, RAG-Sequence-Frozen, RAG-Token, and RAG-Sequence. Performance metrics listed are Exact Match scores for NQ (Natural Questions), TQA (TriviaQA), and WQ (WebQuestions); CT scores, Jeopardy-QGen B-1 and QB-1 scores, MSMarco R-L scores, as well as FVR-3 B-1 and FVR-Label Accuracy. Each cell in the table shows the performance score for a specific configuration on a specific metric.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a MultiVector Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_9144\\2530435390.py:44: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "\n",
    "      doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "\n",
    "      summary_docs = [\n",
    "              Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "              for i, s in enumerate(doc_summaries)\n",
    "          ]\n",
    "\n",
    "      retriever.vectorstore.add_documents(summary_docs)\n",
    "      retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "      # Add texts, tables, and images\n",
    "      # Check that text_summaries is not empty before adding\n",
    "      if text_summaries:\n",
    "          add_documents(retriever, text_summaries, texts)\n",
    "      # Check that table_summaries is not empty before adding\n",
    "      if table_summaries:\n",
    "          add_documents(retriever, table_summaries, tab)\n",
    "      # Check that image_summaries is not empty before adding\n",
    "      if image_summaries:\n",
    "          add_documents(retriever, image_summaries, img)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    Text,\n",
    "    table_summaries,\n",
    "    Table,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiVectorRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002C38B18C070>, docstore=<langchain_core.stores.InMemoryStore object at 0x000002C386CFAB30>, search_kwargs={})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retriever_multi_vector_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f''\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_img_base64(img_base64_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This image is a diagram representing an end-to-end machine learning model designed for natural language processing applications. It illustrates the integration of a query encoder, a non-parametric retriever, a document index, and a parametric generator for tasks such as question answering, fact verification, and question generation. The diagram shows the flow of information from input queries through encoding, retrieval of documents, and output generation, with the process allowing for backpropagation to refine model responses. Visual elements include labeled arrows for data flow, multiple blocks representing different components of the model, and text boxes explaining specific examples of input and output.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "\n",
    "    return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    #print(data_dict)\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are a helpful assistant.\\n\"\n",
    "            \"You will be given a mixed info(s) .\\n\"\n",
    "            \"Use this information to provide relevant information to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(model=\"gpt-4-turbo\", max_tokens=1024)\n",
    "\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: MultiVectorRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002C38B18C070>, docstore=<langchain_core.stores.InMemoryStore object at 0x000002C386CFAB30>, search_kwargs={})\n",
       "           | RunnableLambda(split_image_text_types),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| RunnableLambda(img_prompt_func)\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002C38E0E6A70>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002C38E110310>, root_client=<openai.OpenAI object at 0x000002C386D33E50>, root_async_client=<openai.AsyncOpenAI object at 0x000002C38E0E67D0>, model_name='gpt-4-turbo', model_kwargs={}, openai_api_key=SecretStr('**********'), max_tokens=1024)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_multimodal_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check retrieval\n",
    "query = \"Why We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end?\"\n",
    "docs = retriever_multi_vector_img.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Open-Domain QA Test Scores. For TQA,\\\n",
    "left column uses the standard test set for Open-\\\n",
    "Domain QA, right column uses the TQA-Wiki\\\n",
    "test set. See Appendix D for further details.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever_multi_vector_img.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Models are trained with either 5 or 10 retrieved latent\\\n",
    "documents, and we do not observe significant differences in performance between them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_multi_vector_img.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We get back relevant images\n",
    "plt_img_base64(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"can you explain me this Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\\\n",
    "in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1=\"Explain any images / figures in the paper with Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\\\n",
    "in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI text-based assistant, I do not have the capability to directly view images or figures. However, based on your description, I can provide explanations for how these types of data might typically be presented and analyzed in a scholarly paper:\\n\\n1. **Left: NQ performance as more documents are retrieved.**\\n   - This part of the figure likely portrays how the performance on the Natural Questions (NQ) dataset changes as the number of documents retrieved increases. Generally, the graph could either show improvement in performance metrics such as accuracy or F1 score as more documents provide more information to better respond to queries, or it might depict diminishing returns or even degradation in performance due to increasing noise or irrelevant information.\\n\\n2. **Center: Retrieval recall performance in NQ.**\\n   - This central component would typically show the recall metric, which indicates the proportion of relevant documents that are successfully retrieved out of the total relevant documents available within the dataset as more documents are retrieved. A graph here might show a positive slope indicating that recall generally improves as more documents are considered, revealing the system’s capability to fetch relevant documents.\\n\\n3. **Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.**\\n   - On the right side of the figure, the Bleu-1 and Rouge-L scores for the MS MARCO dataset as more documents are retrieved would be featured. These metrics are used to assess the linguistic quality of generated text compared to reference texts. Blue-1 measures the overlap of single words, while Rouge-L considers the longest common subsequence. The graph might depict how text generation quality (such as in generating answers from retrieved documents) evolves with increasing document retrieval. Performance might increase as more context is available, but similarly to NQ performance, could also plateau or decrease beyond a certain point.\\n\\nThese descriptions provide a typical analysis approach one might expect for such a figure based on the provided labels and usual data handling in natural language processing tasks. For more specific details or interpretations, direct access to the actual figures or additional context from the paper would be required.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run RAG chain\n",
    "chain_multimodal_rag.invoke(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
